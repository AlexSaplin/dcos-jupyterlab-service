{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Waiting for a Spark session to start..."
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "Waiting for a Spark session to start..."
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import logging\n",
    "import argparse\n",
    "import subprocess\n",
    "from imp import reload\n",
    "from tensorflowonspark import TFCluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.addPyFile('TensorFlowOnSpark/examples/mnist/spark/mnist_dist.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mnist_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "reload(logging)\n",
    "logging.basicConfig(format='%(asctime)s %(levelname)s:%(message)s', level=logging.INFO, datefmt='%I:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--epochs\", help=\"number of epochs\", type=int, default=1)\n",
    "parser.add_argument(\"--images\", help=\"HDFS path to MNIST images in parallelized format\")\n",
    "parser.add_argument(\"--labels\", help=\"HDFS path to MNIST labels in parallelized format\")\n",
    "parser.add_argument(\"--format\", help=\"example format\", choices=[\"csv\",\"pickle\",\"tfr\"], default=\"csv\")\n",
    "parser.add_argument(\"--model\", help=\"HDFS path to save/load model during train/test\", default=\"mnist/mnist_csv_model\")\n",
    "parser.add_argument(\"--readers\", help=\"number of reader/enqueue threads\", type=int, default=1)\n",
    "parser.add_argument(\"--steps\", help=\"maximum number of steps\", type=int, default=500)\n",
    "parser.add_argument(\"--batch_size\", help=\"number of examples per batch\", type=int, default=100)\n",
    "parser.add_argument(\"--mode\", help=\"train|inference\", default=\"train\")\n",
    "parser.add_argument(\"--rdma\", help=\"use rdma connection\", default=False)\n",
    "num_executors = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#remove existing models if any\n",
    "subprocess.call([\"hdfs\", \"dfs\", \"-rm\", \"-R\", \"-skipTrash\", \"mnist/mnist_csv_model\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'-rw-r--r--   3 nobody supergroup          0 2018-06-10 20:05 mnist/csv/train/images/_SUCCESS\\n-rw-r--r--   3 nobody supergroup    9338236 2018-06-10 20:05 mnist/csv/train/images/part-00000\\n-rw-r--r--   3 nobody supergroup   11231804 2018-06-10 20:05 mnist/csv/train/images/part-00001\\n-rw-r--r--   3 nobody supergroup   11214784 2018-06-10 20:05 mnist/csv/train/images/part-00002\\n-rw-r--r--   3 nobody supergroup   11226100 2018-06-10 20:05 mnist/csv/train/images/part-00003\\n-rw-r--r--   3 nobody supergroup   11212767 2018-06-10 20:05 mnist/csv/train/images/part-00004\\n-rw-r--r--   3 nobody supergroup   11173834 2018-06-10 20:05 mnist/csv/train/images/part-00005\\n-rw-r--r--   3 nobody supergroup   11214285 2018-06-10 20:05 mnist/csv/train/images/part-00006\\n-rw-r--r--   3 nobody supergroup   11201024 2018-06-10 20:05 mnist/csv/train/images/part-00007\\n-rw-r--r--   3 nobody supergroup   11194141 2018-06-10 20:05 mnist/csv/train/images/part-00008\\n-rw-r--r--   3 nobody supergroup   10449019 2018-06-10 20:05 mnist/csv/train/images/part-00009\\n'\n"
     ]
    }
   ],
   "source": [
    "#verify training images\n",
    "train_images_files = \"mnist/csv/train/images\"\n",
    "print(subprocess.check_output([\"hdfs\", \"dfs\", \"-ls\", \"-R\", train_images_files]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'-rw-r--r--   3 nobody supergroup          0 2018-06-10 20:05 mnist/csv/train/labels/_SUCCESS\\n-rw-r--r--   3 nobody supergroup     204800 2018-06-10 20:05 mnist/csv/train/labels/part-00000\\n-rw-r--r--   3 nobody supergroup     245760 2018-06-10 20:05 mnist/csv/train/labels/part-00001\\n-rw-r--r--   3 nobody supergroup     245760 2018-06-10 20:05 mnist/csv/train/labels/part-00002\\n-rw-r--r--   3 nobody supergroup     245760 2018-06-10 20:05 mnist/csv/train/labels/part-00003\\n-rw-r--r--   3 nobody supergroup     245760 2018-06-10 20:05 mnist/csv/train/labels/part-00004\\n-rw-r--r--   3 nobody supergroup     245760 2018-06-10 20:05 mnist/csv/train/labels/part-00005\\n-rw-r--r--   3 nobody supergroup     245760 2018-06-10 20:05 mnist/csv/train/labels/part-00006\\n-rw-r--r--   3 nobody supergroup     245760 2018-06-10 20:05 mnist/csv/train/labels/part-00007\\n-rw-r--r--   3 nobody supergroup     245760 2018-06-10 20:05 mnist/csv/train/labels/part-00008\\n-rw-r--r--   3 nobody supergroup     229120 2018-06-10 20:05 mnist/csv/train/labels/part-00009\\n'\n"
     ]
    }
   ],
   "source": [
    "#verify training labels\n",
    "train_labels_files = \"mnist/csv/train/labels\"\n",
    "print(subprocess.check_output([\"hdfs\", \"dfs\", \"-ls\", \"-R\", train_labels_files]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Parse arguments for training\n",
    "args = parser.parse_args(['--mode', 'train', '--steps', '3000', '--epochs', '5',\n",
    "                          '--images', train_images_files, \n",
    "                          '--labels', train_labels_files])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08:40:26 INFO:Reserving TFSparkNodes w/ TensorBoard\n",
      "08:40:26 INFO:cluster_template: {'ps': range(0, 1), 'worker': range(1, 5)}\n",
      "08:40:26 INFO:listening for reservations at ('10.0.3.39', 35517)\n",
      "08:40:26 INFO:Starting TensorFlow on executors\n",
      "08:40:26 INFO:Waiting for TFSparkNodes to start\n",
      "08:40:26 INFO:waiting for 5 reservations\n",
      "08:40:27 INFO:waiting for 5 reservations\n",
      "08:40:28 INFO:waiting for 5 reservations\n",
      "08:40:29 INFO:waiting for 5 reservations\n",
      "08:40:30 INFO:waiting for 3 reservations\n",
      "08:40:31 INFO:all reservations completed\n",
      "08:40:31 INFO:All TFSparkNodes started\n",
      "08:40:31 INFO:{'executor_id': 2, 'host': '10.0.0.69', 'job_name': 'worker', 'task_index': 1, 'port': 34437, 'tb_pid': 0, 'tb_port': 0, 'addr': '/tmp/pymp-yfhjek51/listener-0vsjzptz', 'authkey': b'\\xef\\xfc\\xf5\\xc9\\x18\\x9dL\\x02\\x96yv\\xa70\\x85\\xe4\\xab'}\n",
      "08:40:31 INFO:{'executor_id': 4, 'host': '10.0.2.241', 'job_name': 'worker', 'task_index': 3, 'port': 42945, 'tb_pid': 0, 'tb_port': 0, 'addr': '/tmp/pymp-cxco13ug/listener-hq8zaynu', 'authkey': b'\\xa5\\xe5D\\xc6\\x8cbK\\xc0\\x87Q\\xe0\\xec\\xe6u9\\xd2'}\n",
      "08:40:31 INFO:{'executor_id': 1, 'host': '10.0.3.126', 'job_name': 'worker', 'task_index': 0, 'port': 42791, 'tb_pid': 118, 'tb_port': 42603, 'addr': '/tmp/pymp-lbh8enpd/listener-8va9mjch', 'authkey': b'#\\x88\\xfd\\xd5\\x83\\xe7OA\\xb5b\\x10F{u\\xff\\xa4'}\n",
      "08:40:31 INFO:{'executor_id': 3, 'host': '10.0.0.131', 'job_name': 'worker', 'task_index': 2, 'port': 34667, 'tb_pid': 0, 'tb_port': 0, 'addr': '/tmp/pymp-tkd384ec/listener-8ukfv3yi', 'authkey': b'\\x12\\xb8\\x80V\\xc9\\x06G(\\x8d\\x11<J\"n\\xbc\\x8d'}\n",
      "08:40:31 INFO:{'executor_id': 0, 'host': '10.0.1.7', 'job_name': 'ps', 'task_index': 0, 'port': 41131, 'tb_pid': 0, 'tb_port': 0, 'addr': ('10.0.1.7', 35567), 'authkey': b'\\x14\\n\\xe9\\xd9\\x97\\xd4I-\\xa0a]\\x96S\\xb6u\\xb4'}\n",
      "08:40:31 INFO:========================================================================================\n",
      "08:40:31 INFO:\n",
      "08:40:31 INFO:TensorBoard running at:       http://10.0.3.126:42603\n",
      "08:40:31 INFO:\n",
      "08:40:31 INFO:========================================================================================\n"
     ]
    }
   ],
   "source": [
    "#start the cluster for training\n",
    "cluster = TFCluster.run(sc, mnist_dist.map_fun, args, num_executors, 1, True, TFCluster.InputMode.SPARK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08:40:33 INFO:Feeding training data\n"
     ]
    }
   ],
   "source": [
    "#Feed data via Spark RDD\n",
    "images = sc.textFile(args.images).map(lambda ln: [int(x) for x in ln.split(',')])\n",
    "labels = sc.textFile(args.labels).map(lambda ln: [float(x) for x in ln.split(',')])\n",
    "dataRDD = images.zip(labels)\n",
    "cluster.train(dataRDD, args.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cluster.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print(subprocess.check_output([\"hdfs\", \"dfs\", \"-ls\", \"mnist/mnist_csv_model\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#verify test images\n",
    "test_images_files = \"mnist/csv/test/images\"\n",
    "print(subprocess.check_output([\"hdfs\", \"dfs\", \"-ls\", test_images_files]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#verify test labels\n",
    "test_labels_files = \"mnist/csv/test/labels\"\n",
    "print(subprocess.check_output([\"hdfs\", \"dfs\", \"-ls\", test_labels_files]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Parse arguments for inference\n",
    "args = parser.parse_args(['--mode', 'inference', \n",
    "                          '--images', test_images_files, \n",
    "                          '--labels', test_labels_files])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Start the cluster for inference\n",
    "cluster = TFCluster.run(sc, mnist_dist.map_fun, args, num_executors, 1, False, TFCluster.InputMode.SPARK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#prepare data as Spark RDD\n",
    "images = sc.textFile(args.images).map(lambda ln: [int(x) for x in ln.split(',')])\n",
    "labels = sc.textFile(args.labels).map(lambda ln: [float(x) for x in ln.split(',')])\n",
    "dataRDD = images.zip(labels)\n",
    "#feed data for inference\n",
    "prediction_results = cluster.inference(dataRDD)\n",
    "prediction_results.take(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "cluster.shutdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - PySpark",
   "language": "python",
   "name": "apache_toree_pyspark"
  },
  "language_info": {
   "codemirror_mode": "text/x-ipython",
   "file_extension": ".py",
   "mimetype": "text/x-ipython",
   "name": "python",
   "pygments_lexer": "python",
   "version": "3.6.5\n"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
